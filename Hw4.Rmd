---
title: "EconHw4"
author: "Trevor Freeland"
date: "May 6, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, comment = NA, warning = F)
```

```{r, include = F}
library(tidyverse)
library(stargazer)
library(pander)
library(lubridate)
library(Quandl)
library(readxl)
library(car)
library(lmtest)
```

```{r, data}
df <- read_excel("~/R/Econometrics/Tables/Table2_8.xls", skip=2)

data1 <- read.table("~/R/Stats345/EconHw4/EconData.txt")

DPI <- Quandl("FRED/A067RC1A027NBEA")
names(DPI) <- c("Date", "DPI")
Sav <- Quandl("FRED/A071RC1A027NBEA")
names(Sav) <- c("Date", "Savings")
Q2data <- left_join(DPI, Sav)
Q2data$Year <- year(Q2data$Date)
Q2data$Period <- ifelse(Q2data$Year <= 2007, 0, 1)
Q2data <- Q2data %>% filter(Year >= 1998)
```

##1

```{r, results = 'hide'}
#Confirm I get the same results as them
model1 <- lm(V9~V6+V2, data = data1)
summary(model1) # I did
```

In order to modify the model in 9.3.1 to find out if there was an interaction between gender and region I would need to add in the gender variable and make sure to include an interaction between gender and region. In R this means I would have a multiplication sign in between the two variables. 

9.3.1: V9 = 8.8148 + 1.0997V6 − 1.6729V2

New model: V9 = 9.74 + 1.13V6 − 1.36V2  -2.00V5  -.80(V2:V5)

V9 = hourly wage ($)
V6= married status; 1 = married, 0 = otherwise
V2 = region of residence; 1 = South, 0 = otherwise
V5 = gender (1 = Female, 0 = Male)

Our intercept has increased in the new model. From our output below we can see that the gender variable is definitely significant however the interaction between gender and region does not appear to be significant. Our estimates for our other variables in the model has changed as well. In our new model married status seems to have a slightly larger effect than in the original model, and region appears to have slightly less of an effect in our new model. 

See table below for summary of model. 

```{r}
model2 <- lm(V9~V6+V2*V5, data = data1)
pander(summary(model2))
```

##2

###(A)

In our table below we show the differences between the two models. The only slight indication that the model with log(Savings) as the response has slightly higher adjusted R-squared, which might point to it being the correct model. When examinging the residual plots, neither of them looked particularly better than the other so I did not include them in this report. 

In the second code chunk below we ran the MWD test. We created a new Z variable as per the instructions of the MWD test and ran a regression including the new Z variable. The Z variable does not appear to be significant which is an indicator that the true model is linear. 

```{r}
model.book <- lm(Savings~Period*DPI, data = Q2data)
model.q <- lm(log(Savings)~Period*DPI, data = Q2data)
stargazer(model.book, model.q, type="text")
```

```{r}
#MWD Test
Q2data$z <- log(440 + 162*Q2data$Period -.011 * Q2data$DPI + .0177*(Q2data$DPI * Q2data$Period)) - (6.12 + .034*Q2data$Period -.0000343*Q2data$DPI + .0000386*(Q2data$Period * Q2data$DPI))
model.test <- lm(Savings~Period*DPI + z, data = Q2data)
stargazer(model.test, type="text")
#the z estimate is not significant, therefore 
#We do not reject hypo that the true model is 
#linear. 
```


###(B)

We can interpret the .336 estimate of the coefficient on the dummy variable in this model as the late period (2008-2017) has a 33.6% increase in intercept from the earlier period. 

```{r, results ='hide'}
summary(model.q)
```

###(C)

It does not appear that the residual standard errors are statistically difference, .179 and .206, with 8 degrees of freedom. THis implies that we could use the Chow test to pool the data together, however we saw that we can also do this with using a dummy variable.

```{r}
model.old <- lm(log(Savings)~DPI, data = Q2data[11:20,])
model.new <- lm(log(Savings)~DPI, data = Q2data[1:10,])

stargazer(model.old, model.new, type = "text")
```

##3

###(A)

a. Regress expenditure on food on total expenditure, and examine the residuals
obtained from this regression.

Based on the plot below, there appears to be a lot of variablilty in our residuals. There also might be heteroscedasticity, but since we are only plotting against the index we can't tell that for sure. 

```{r}
india.lm <- lm(FOODEXP~TOTALEXP, data = df)
plot(resid(india.lm), xlab = "Index", ylab = "Residuals", main = "Residual Plot")
```

###(B)

b. Plot the residuals obtained in (a) against total expenditure and see if you observe
any systematic pattern.

Now that we are plotting against the total expenditure we can see that there definitely is some heteroscedaticity in our data. As Total expenditure increases the residuals spread out more, which is a prime indicator of heteroscedaticity.

```{r}
plot(resid(india.lm)~df$TOTALEXP, xlab = "Total Expenditure", ylab = "Residuals", main = "Residuals against Total Expenditure")
```

###(C)

c. If the plot in (b) suggests that there is heteroscedasticity, apply the Park, Glejser,
and White tests to find out if the impression of heteroscedasticity observed in (b)
is supported by these tests.

All three of the tests provided evidence that there is heteroscedasticity present in our model/data. This means that the three tests support the impression of heteroscedasticity that we observed in part (b).

```{r, results = 'hide'}
#Park Test
summary(india.lm)
df$u <- df$FOODEXP - 94.2 + .437*df$TOTALEXP 
park.lm <- lm(log(u^2)~log(TOTALEXP), data = df)
summary(park.lm) #coefficient on Log(X) is significant
#indicates heteroscedasticity.

#Glejser Test
glejser.lm <- lm(abs(u)~TOTALEXP, data = df)
summary(glejser.lm) #COeff is significant again
#indicates heteroscedasticity

#White = bptest
bptest(india.lm) #Small p-value again
#indicates heteroscedasticity
```

###(D)

d. Obtain White’s heteroscedasticity-consistent standard errors and compare those
with the OLS standard errors. Decide if it is worth correcting for heteroscedasticity
in this example.

Suprisingly the standard errors for White's heteroscedasticity-consistent standard errors are not that different from the ones we get when not accounting for heteroscedasticty. In the table below you can see the values side by side and can tell that the changes in standard error are very small and don't change the significance in any of our terms so may not want to worry about correcting for the heteroscedasticity since it doesn't seem to be changing anything significantly. 

```{r}
#coeftest(india.lm, hccm(india.lm))
stargazer(india.lm, coeftest(india.lm, hccm(india.lm)), type="text")
```

##4

11.17. Repeat Exercise 11.16, but this time regress the logarithm of expenditure on food
on the logarithm of total expenditure. If you observe heteroscedasticity in the linear
model of Exercise 11.16 but not in the log–linear model, what conclusion do you
draw? Show all the necessary calculations.



##5


